{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tugas Akhir JST - Seeds .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-1gLXK1zAPz"
      },
      "source": [
        "'''binary encoding'''\n",
        "def bin_enc(lbl):\n",
        "  mi = min(lbl)\n",
        "  length = len(bin(max(lbl)-mi+1)[2:])\n",
        "  enc=[]\n",
        "\n",
        "  for i in lbl:\n",
        "    b=bin(i-mi)[2:].zfill(length)\n",
        "    enc.append([int(n) for n in b])\n",
        "\n",
        "  return enc  \n",
        "\n",
        "def bin_dec(enc, mi=0):\n",
        "  lbl=[]\n",
        "\n",
        "  for e in enc:\n",
        "    rounded=[int(round(x)) for x in e]\n",
        "    string= ''.join(str(x) for x in rounded)\n",
        "    num=int(string,2) + mi\n",
        "    lbl.append(num)\n",
        "    \n",
        "  return lbl\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBVIuDpu_oTn"
      },
      "source": [
        "#one-hot encoding\n",
        "\n",
        "import numpy as np\n",
        "def onehot_enc(lbl, min_val=0):\n",
        "  mi=min(lbl)\n",
        "  enc=np.full((len(lbl),max(lbl)-mi+1), min_val, np.int8)\n",
        "  for i, x in enumerate(lbl):\n",
        "    enc[i, x-mi]=1\n",
        "  return enc\n",
        "\n",
        "def onehot_dec(enc, mi=0):\n",
        "  return [np.argmax(e)+mi for e in enc]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhKH6ijk6l1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1812d55c-3d04-45a9-9691-e3288c398736"
      },
      "source": [
        "labels= 1, 2, 3, 4, 5\n",
        "enc = onehot_enc(labels)\n",
        "dec = onehot_dec(enc, min(labels))\n",
        "\n",
        "print(enc)\n",
        "print(dec)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 0 1 0]\n",
            " [0 0 0 0 1]]\n",
            "[1, 2, 3, 4, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGmWsez87tA5"
      },
      "source": [
        "'''Fungsi aktivasi sigmoid dan turunannya'''\n",
        "\n",
        "def sig(X):\n",
        " return [1/(1+np.exp(-x)) for x in X]\n",
        "\n",
        "def sigd(X):\n",
        " output=[]\n",
        " for i, x in enumerate(X):\n",
        "   s = sig([x])[0]\n",
        "   output.append(s*(1-s))\n",
        "\n",
        " return output\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx3-vv_g8IxG"
      },
      "source": [
        "'''fungsi modeling (training backpropagation)'''\n",
        "def bp_fit(X, target, layer_conf, max_epoch, max_error=.1, learn_rate=.1,print_per_epoch=100):\n",
        "  nin=[np.empty(i) for i in layer_conf]\n",
        "\n",
        "  n = [np.empty(j+1) if i<len(layer_conf)-1\n",
        "      else np.empty(j) for i, j in enumerate(layer_conf)]\n",
        "      \n",
        "  w = np.array([np.random.rand(layer_conf[i]+1, layer_conf[i+1])\n",
        "                for i in range(len(layer_conf)-1)])\n",
        "  \n",
        "  dw = [np.empty((layer_conf[i]+1, layer_conf[i+1]))\n",
        "        for i in range(len(layer_conf)-1)]\n",
        "        \n",
        "  d = [np.empty(s) for s in layer_conf[1:]]\n",
        "  din = [np.empty(s) for s in layer_conf[1:-1]]\n",
        "  epoch = 0\n",
        "  mse = 1\n",
        "\n",
        "  for i in range(0, len(n)-1):\n",
        "    n[i][-1]=1\n",
        "  while (max_epoch == -1 or epoch<max_epoch) and mse>max_error:\n",
        "    epoch +=1\n",
        "    mse = 0\n",
        "    for r in range(len(X)):\n",
        "      n[0][:-1]=X[r]\n",
        "\n",
        "      for L in range(1, len(layer_conf)):\n",
        "        nin[L] = np.dot(n[L-1], w[L-1])\n",
        "        n[L][:len(nin[L])]=sig(nin[L])\n",
        "\n",
        "      e = target[r] - n[-1]\n",
        "      mse += sum(e ** 2)\n",
        "      d[-1]=e*sigd(nin[-1])\n",
        "      dw[-1]=learn_rate * d[-1]*n[-2].reshape((-1,1))\n",
        "\n",
        "      for L in range(len(layer_conf)-1, 1, -1):\n",
        "        din[L-2]=np.dot(d[L-1], np.transpose(w[L-1][:-1]))\n",
        "        d[L-2]=din[L-2]*np.array(sigd(nin[L-1]))\n",
        "        dw[L-2]=(learn_rate*d[L-2])*n[L-2].reshape((-1,1))\n",
        "\n",
        "      w += dw\n",
        "    mse /= len(X)\n",
        "\n",
        "  if print_per_epoch > -1 and epoch % print_per_epoch == 0:\n",
        "    print(f'Epoch {epoch}, MSE: {mse}')\n",
        "\n",
        "  return w, epoch, mse\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmyJTzxW9dWf"
      },
      "source": [
        "'''fungsi pengujian back propagation'''\n",
        "\n",
        "def bp_predict(X,w):\n",
        "  n=[np.empty(len(i)) for i in w]\n",
        "  nin=[np.empty(len(i[0])) for i in w]\n",
        "  predict = []\n",
        "  n.append(np.empty(len(w[-1][0])))\n",
        "\n",
        "  for x in X:\n",
        "    n[0][:-1]=x\n",
        "\n",
        "    for L in range(0, len(w)):\n",
        "      nin[L] = np.dot(n[L], w[L])\n",
        "      n[L+1][:len(nin[L])] = sig(nin[L])\n",
        "\n",
        "    predict.append(n[-1].copy())\n",
        "  \n",
        "  return predict\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvKAjbA-LWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3673f96-4a16-4888-92fb-0d249a170e8d"
      },
      "source": [
        "# Import library\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data, pisahkan data dan label\n",
        "\n",
        "#iris = datasets.load_iris()\n",
        "#X = minmax_scale(iris.data)\n",
        "#Y = onehot_enc(iris.target)\n",
        "seeds_dataset = np.loadtxt('seeds_dataset.txt')\n",
        "\n",
        "data = seeds_dataset[:, :7]\n",
        "labels = seeds_dataset[:, 7].reshape((data.shape[0]))\n",
        "\n",
        "label = []\n",
        "for i in range (len(labels)) :\n",
        "  label.append(int(labels[i]))\n",
        "\n",
        "X = minmax_scale(data)\n",
        "Y = onehot_enc(label)\n",
        "\n",
        "# Preprocess data: membagi data training, label training dan data testing, label testing (2/3 data training, 1/3 data testing)\n",
        "# Atau dengan kata lain Memisahkan data training, data testing, label training, label testing (2/3 data training, 1/3 data testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.3)\n",
        "\n",
        "# Membangun model JST / backpropagation menggunakan data training dan label training\n",
        "\n",
        "w, ep, mse = bp_fit(X_train, y_train, layer_conf=(7,10,3),learn_rate=.1, max_epoch=1000, max_error=.1, print_per_epoch=25)\n",
        "\n",
        "print(f'Epochs: {ep}, MSE: {mse}')\n",
        "\n",
        "# Menguji model / Menguji kinerja backpropagation menggunakan data testing\n",
        "predict = bp_predict(X_test, w)\n",
        "predict = onehot_dec(predict)\n",
        "\n",
        "predict1 = []\n",
        "for i in range (len(predict)):\n",
        "  predict1.append(predict[i]+1)\n",
        "\n",
        "#Membandingkan luaran dengan label testing / target\n",
        "y_test = onehot_dec(y_test)\n",
        "acc = accuracy_score(predict, y_test)\n",
        "\n",
        "y_test1 = []\n",
        "for i in range (len(y_test)):\n",
        "  y_test1.append(y_test[i]+1)\n",
        "\n",
        "print(len(X_test))\n",
        "print(len(X_train))\n",
        "print(f'Output: {predict1}')\n",
        "print(f'True: {y_test1}')\n",
        "print(f'Accuracy: {acc}')"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 208, MSE: 0.09979312296157221\n",
            "63\n",
            "147\n",
            "Output: [2, 1, 3, 3, 3, 1, 1, 3, 1, 2, 3, 1, 2, 3, 3, 2, 2, 1, 2, 3, 1, 2, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 3, 1, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 1, 2, 1, 2, 3, 3, 1, 3, 1, 1, 3, 2]\n",
            "True: [2, 1, 3, 3, 3, 1, 1, 3, 1, 2, 3, 1, 2, 3, 3, 2, 2, 1, 2, 3, 1, 2, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 3, 1, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 2, 1, 2, 1, 2, 3, 3, 1, 3, 1, 1, 3, 2]\n",
            "Accuracy: 0.9841269841269841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_D2GLrrFm1d",
        "outputId": "33272569-70c7-41f5-9c83-692a794654db"
      },
      "source": [
        "'''Menguji label'''\n",
        "\n",
        "from sklearn import datasets\n",
        "seeds_dataset = np.loadtxt('seeds_dataset.txt')\n",
        "\n",
        "dataz = seeds_dataset[:, :7]\n",
        "labels = seeds_dataset[:, 7]\n",
        "\n",
        "label = []\n",
        "for i in range (len(labels)) :\n",
        "  label.append(int(labels[i]))\n",
        "\n",
        "#print(dataz)\n",
        "print(label)\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = minmax_scale(iris.data)\n",
        "Y = onehot_enc(label)\n",
        "\n",
        "Y1 = onehot_dec(Y)\n",
        "Y2 = []\n",
        "\n",
        "#print(onehot_enc(label))\n",
        "for i in range (len(Y1)):\n",
        "  Y2.append(Y1[i]+1)\n",
        "\n",
        "print(Y2)"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYIUV_eWRyF0",
        "outputId": "3eced3e9-d07a-4aca-a9d6-d18f2a416d64"
      },
      "source": [
        "'''Jika ingin mengelompokkan data test dan data trainibng sendiri'''\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "X = np.array(data)\n",
        "y = np.array(label)\n",
        "kf = KFold(n_splits=3,shuffle=True)\n",
        "\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}